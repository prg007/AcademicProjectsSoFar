{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 (Part 3): Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Civilian, what's your twenty on Operation Heroic Storm?\" asks Colonel Trick.\n",
    "\n",
    "\"I thought it was called Operation Heroic Deliverance,\" you say.\n",
    "\n",
    "\"Escalation, civilian,\" says Colonel Trick. \"Escalation.\"\n",
    "\n",
    "\"And also, 'what's your twenty' is a miliary thing? I thought it was more a police thing.\"\n",
    "\n",
    "\"Just testing you, civilian.\"\n",
    "\n",
    "\"Or a Call of Duty thing.\"\n",
    "\n",
    "\"What's your status, civilian?\" asks Colonel Trick.\n",
    "\n",
    "\"Well,\" you say proudly. \"We've gotten both the data and the basic neural network training infrastructure set up, and...\"\n",
    "\n",
    "\"Neural networks?\" asks Trick dubiously. \"I'd heard from you lot that support vector machines were the way to go.\"\n",
    "\n",
    "\"Well, sir,\" you say. \"That used to be the case, but people have been getting more success from neural networks these days.\"\n",
    "\n",
    "\"You scientists,\" mutters Colonel Trick. \"Always vacillating.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back at Dulles International Airport, waiting again for your flight home, CNN is playing on all the monitors. The top story is the emergence of a cult of humans that have joined the zebras in their international misadventures.\n",
    "\n",
    "![anderson cooper](./img/cooper2.png)\n",
    "\n",
    "This inspires you to start coding up your convolutional neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You consult your notes on Computing Convolutions. The first step to building a convolutional layer is to take the kernels and convert them into a **kernel-row matrix**. For instance, suppose you have two 2x2 RGB (three channel) kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "red_kernel1 = torch.tensor([[1., 2], [3, 4]])\n",
    "green_kernel1 = torch.tensor([[5., 6], [7, 8]])\n",
    "blue_kernel1 = torch.tensor([[9., 10], [11, 12]])\n",
    "kernel1 = torch.stack([red_kernel1, green_kernel1, blue_kernel1])\n",
    "print(\"THE FIRST KERNEL (with shape {}):\".format(kernel1.shape))\n",
    "print(kernel1)\n",
    "\n",
    "red_kernel2 = torch.tensor([[13., 14], [15, 16]])\n",
    "green_kernel2 = torch.tensor([[17., 18], [19, 20]])\n",
    "blue_kernel2 = torch.tensor([[21., 22], [23, 24]])\n",
    "kernel2 = torch.stack([red_kernel2, green_kernel2, blue_kernel2])\n",
    "print(\"\\nTHE SECOND KERNEL (with shape {}):\".format(kernel2.shape))\n",
    "print(kernel2)\n",
    "\n",
    "\n",
    "kernels = torch.stack([kernel1, kernel2])\n",
    "print(\"\\nTHE KERNEL TENSOR (SHAPE {}):\".format(kernels.shape))\n",
    "print(kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kernel-row matrix converts each kernel into one row of a matrix, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_row_matrix = torch.tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.],\n",
    "                                  [13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24.]])\n",
    "print(\"\\nTHE KERNEL-ROW MATRIX (SHAPE {}):\".format(kernel_row_matrix.shape))\n",
    "print(kernel_row_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the implementation of ```create_kernel_row_matrix``` in ```cnn.py```.\n",
    "\n",
    "We've provided a unit test in test.py, so that you can (a) understand the expected behavior  and (b) check your implementation is working properly. Run it from the command line as follows:\n",
    "\n",
    "    python -m unittest test.CnnTests.test_kernel_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to take each window of a batch of input images and convert them into columns of a **window-column matrix**. For instance, suppose you have two 3x3 RGB (three channel) images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red1 = torch.tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n",
    "green1 = torch.tensor([[10., 11, 12], [13,14,15], [16,17,18]])\n",
    "blue1 = torch.tensor([[19.,20,21], [22,23,24], [25,26,27]])\n",
    "rgb1 = torch.stack([red1, green1, blue1])\n",
    "print(\"THE FIRST IMAGE (with shape {}):\".format(rgb1.shape))\n",
    "print(rgb1)\n",
    "\n",
    "\n",
    "red2 = torch.tensor([[28., 29, 30], [31,32,33], [34,35,36]])\n",
    "green2 = torch.tensor([[37., 38, 39], [40,41,42], [43,44,45]])\n",
    "blue2 = torch.tensor([[46.,47,48], [49,50,51], [52,53,54]])\n",
    "rgb2 = torch.stack([red2, green2, blue2])\n",
    "print(\"THE SECOND IMAGE (with shape {}):\".format(rgb2.shape))\n",
    "print(rgb2)\n",
    "\n",
    "images = torch.stack([rgb1, rgb2])\n",
    "print(\"\\nTHE IMAGE TENSOR (SHAPE {}):\".format(images.shape))\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A window-column matrix converts each window of each image into a column of a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_column_matrix = torch.tensor([   [ 1.,  2.,  4.,  5., 28., 29., 31., 32.],\n",
    "                                        [ 2.,  3.,  5.,  6., 29., 30., 32., 33.],\n",
    "                                        [ 4.,  5.,  7.,  8., 31., 32., 34., 35.],\n",
    "                                        [ 5.,  6.,  8.,  9., 32., 33., 35., 36.],\n",
    "                                        [10., 11., 13., 14., 37., 38., 40., 41.],\n",
    "                                        [11., 12., 14., 15., 38., 39., 41., 42.],\n",
    "                                        [13., 14., 16., 17., 40., 41., 43., 44.],\n",
    "                                        [14., 15., 17., 18., 41., 42., 44., 45.],\n",
    "                                        [19., 20., 22., 23., 46., 47., 49., 50.],\n",
    "                                        [20., 21., 23., 24., 47., 48., 50., 51.],\n",
    "                                        [22., 23., 25., 26., 49., 50., 52., 53.],\n",
    "                                        [23., 24., 26., 27., 50., 51., 53., 54.]])\n",
    "print(\"THE WINDOW_COLUMN MATRIX (SHAPE {}):\".format(window_column_matrix.shape))\n",
    "print(window_column_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in the above matrix corresponds to a 3x2x2 window of an image (the first 4 columns are the windows of image 1, the second 4 columns are the windows of image 2). Each window is **3**x2x2, because there are 3 (RGB) channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the implementation of ```create_window_column_matrix``` in ```cnn.py```.\n",
    "\n",
    "We've provided two unit tests in test.py, so that you can (a) understand the expected behavior  and (b) check your implementation is working properly. Run them from the command line as follows:\n",
    "\n",
    "    python -m unittest test.CnnTests.test_window_column1\n",
    "\n",
    "and\n",
    "\n",
    "    python -m unittest test.CnnTests.test_window_column2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third thing you'll need is a function to add a padding of zeroes to each image in a batch. For instance, given the images defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTHE IMAGE TENSOR (SHAPE {}):\".format(images.shape))\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A padding of 2 would result in each image having a border of two zeros in each direction, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "\n",
    "t = tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  1.,  2.,  3.,  0.,  0.],\n",
    "              [ 0.,  0.,  4.,  5.,  6.,  0.,  0.],\n",
    "              [ 0.,  0.,  7.,  8.,  9.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "             [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0., 10., 11., 12.,  0.,  0.],\n",
    "              [ 0.,  0., 13., 14., 15.,  0.,  0.],\n",
    "              [ 0.,  0., 16., 17., 18.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "             [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0., 19., 20., 21.,  0.,  0.],\n",
    "              [ 0.,  0., 22., 23., 24.,  0.,  0.],\n",
    "              [ 0.,  0., 25., 26., 27.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]]],\n",
    "\n",
    "\n",
    "            [[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0., 28., 29., 30.,  0.,  0.],\n",
    "              [ 0.,  0., 31., 32., 33.,  0.,  0.],\n",
    "              [ 0.,  0., 34., 35., 36.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "             [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0., 37., 38., 39.,  0.,  0.],\n",
    "              [ 0.,  0., 40., 41., 42.,  0.,  0.],\n",
    "              [ 0.,  0., 43., 44., 45.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "             [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0., 46., 47., 48.,  0.,  0.],\n",
    "              [ 0.,  0., 49., 50., 51.,  0.,  0.],\n",
    "              [ 0.,  0., 52., 53., 54.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]]]])\n",
    "\n",
    "print(\"\\nTHE PADDED IMAGE TENSOR (SHAPE {}):\".format(t.shape))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the implementation of ```pad``` in ```cnn.py```.\n",
    "\n",
    "We've provided a unit test in test.py, so that you can (a) understand the expected behavior  and (b) check your implementation is working properly. Run it from the command line as follows:\n",
    "\n",
    "    python -m unittest test.CnnTests.test_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these three functions working, you can go ahead and implement a function to convolve a set of kernels with a batch of images, using the techniques described in your Computing Convolutions notes. Convolving our two example kernels with our two 3x3 example images (with stride 1 and no padding), should yield the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be: result = convolve(kernels, images, stride=1, padding=0)\n",
    "result = tensor([[[   [1245., 1323.],\n",
    "                      [1479., 1557.]],\n",
    "\n",
    "                     [[2973., 3195.],\n",
    "                      [3639., 3861.]]],\n",
    "\n",
    "\n",
    "                    [[[3351., 3429.],\n",
    "                      [3585., 3663.]],\n",
    "\n",
    "                     [[8967., 9189.],\n",
    "                      [9633., 9855.]]]])\n",
    "\n",
    "print(\"\\nTHE RESULT OF THE CONVOLUTION (SHAPE {}):\".format(result.shape))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the implementation of ```convolve``` in ```cnn.py```.\n",
    "\n",
    "We've provided a unit test in test.py, so that you can (a) understand the expected behavior  and (b) check your implementation is working properly. Run it from the command line as follows:\n",
    "\n",
    "    python -m unittest test.CnnTests.test_conv\n",
    "    \n",
    "Once convolve is operational, the ```torch.nn.Module``` called ```ConvLayer``` in ```cnn.py``` should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT IMAGE TENSOR (SHAPE torch.Size([2, 3, 4, 4])):\n",
      "\n",
      "RESULT OF CONVOLUTION (SHAPE torch.Size([2, 1, 4, 4])):\n",
      "tensor([[[[-1.0724,  0.2315, -1.4380,  1.0853],\n",
      "          [-0.6878, -0.8480, -1.3513,  0.4508],\n",
      "          [ 0.8623,  0.0679,  0.5354,  1.7838],\n",
      "          [-0.1533, -0.5266, -0.6946,  0.3432]]],\n",
      "\n",
      "\n",
      "        [[[-3.3885,  1.3807,  0.4243,  2.0232],\n",
      "          [-2.1855, -0.5170, -0.6447,  0.9560],\n",
      "          [-0.0981, -0.3739,  1.3703,  3.5306],\n",
      "          [-0.7753, -1.3160, -0.3610,  0.7642]]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from cnn import ConvLayer\n",
    "from test import construct_test_images2\n",
    "\n",
    "conv = ConvLayer(input_channels=3, num_kernels=1, \n",
    "                 kernel_size=3,\n",
    "                 stride=1, \n",
    "                 padding=1)\n",
    "\n",
    "images = construct_test_images2()\n",
    "# print(images)\n",
    "print(\"INPUT IMAGE TENSOR (SHAPE {}):\".format(images.shape))\n",
    "convolved = conv.forward(images)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nRESULT OF CONVOLUTION (SHAPE {}):\".format(convolved.shape))\n",
    "print(convolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You should understand the structure of these tensors. The input tensor ```images``` has shape ```(2,3,4,4)``` which means that the batch has two 4x4 images, each with 3 channels (so they're RGB images). The output tensor ```convolved``` has shape ```(2,1,4,4)```, because the 1 kernel has produced a 4x4 matrix for each of the two batch images.\n",
    "\n",
    "We can now run the result through a ReLU layer (since we made ReLU layers last time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT OF RELU (SHAPE torch.Size([2, 1, 4, 4])):\n",
      "tensor([[[[0.0000, 0.2315, 0.0000, 1.0853],\n",
      "          [0.0000, 0.0000, 0.0000, 0.4508],\n",
      "          [0.8623, 0.0679, 0.5354, 1.7838],\n",
      "          [0.0000, 0.0000, 0.0000, 0.3432]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 1.3807, 0.4243, 2.0232],\n",
      "          [0.0000, 0.0000, 0.0000, 0.9560],\n",
      "          [0.0000, 0.0000, 1.3703, 3.5306],\n",
      "          [0.0000, 0.0000, 0.0000, 0.7642]]]], grad_fn=<ClampBackward>)\n"
     ]
    }
   ],
   "source": [
    "from training import ReLU\n",
    "relu = ReLU()\n",
    "result = relu.forward(convolved)\n",
    "print(\"\\nRESULT OF RELU (SHAPE {}):\".format(result.shape))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result of applying ReLU has the same shape, because ReLU simply applies the activation function elementwise.\n",
    "\n",
    "If we want to feed these new evidence variables as input to a standard feedforward layer (i.e. ```Dense``` from last time), the problem is that the tensors have order 4, whereas ```Dense``` expects tensors of order 2, i.e. the shape should be ```(batch_size, num_evidence_vars)```.\n",
    "\n",
    "Right now, the result of ReLU has shape ```(batch_size, num_kernels, image_width, image_width)```. So we need to flatten that tensor into the right shape, because at this point, all the evidence variables per batch can be treated identically, so we don't need the additional structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the implementation of the ```Flatten``` module in ```cnn.py```.\n",
    "\n",
    "We've provided a unit test in test.py, so that you can (a) understand the expected behavior  and (b) check your implementation is working properly. Run it from the command line as follows:\n",
    "\n",
    "    python -m unittest test.CnnTests.test_flatten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this infrastructure in place, you proceed to create a convolutional neural network with two convolutional layers and two feedforward layers. Note that you've made it general enough to handle both grayscale images (with 1 input channel) and RGB images (with 3 input channels).\n",
    "\n",
    "For the convolutional layers, you set the padding and stride so that the size of the input and output images are the same.\n",
    "\n",
    "Unfortunately, you ran out of the time at the airport to implement a maxpool layer, but luckily the folks at Torch have an implementation (torch.nn.MaxPool2d) that you can leverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from training import ReLU, Dense\n",
    "from torch.nn import MaxPool2d, Conv2d\n",
    "from cnn import ConvLayer, Flatten\n",
    "from cnn import Classifier\n",
    "\n",
    "def create_cnn(num_kernels, kernel_size, \n",
    "               output_classes, dense_hidden_size,\n",
    "               image_width, is_grayscale=True,\n",
    "               use_torch_conv_layer = True,\n",
    "               use_maxpool=True):\n",
    "    \n",
    "    if use_torch_conv_layer:\n",
    "        Conv = Conv2d\n",
    "    else:\n",
    "        Conv = ConvLayer    \n",
    "    padding = kernel_size//2\n",
    "    output_width = image_width\n",
    "    if use_maxpool:\n",
    "        output_width = output_width // 16\n",
    "    model = torch.nn.Sequential()\n",
    "    if is_grayscale:\n",
    "        num_input_channels = 1\n",
    "    else:\n",
    "        num_input_channels = 3\n",
    "    model.add_module(\"conv1\", Conv(num_input_channels, num_kernels,\n",
    "                                   kernel_size=kernel_size, \n",
    "                                   stride=1, padding=padding))\n",
    "    model.add_module(\"relu1\", ReLU())\n",
    "    if use_maxpool:\n",
    "        model.add_module(\"pool1\", MaxPool2d(kernel_size=4, stride=4, padding=0))\n",
    "    model.add_module(\"conv2\", Conv(num_kernels, num_kernels,\n",
    "                                              kernel_size=kernel_size, \n",
    "                                              stride=1, padding=padding))\n",
    "    model.add_module(\"relu2\", ReLU())\n",
    "    if use_maxpool:\n",
    "        model.add_module(\"pool2\", MaxPool2d(kernel_size=4, stride=4, padding=0))\n",
    "    model.add_module(\"flatten\", Flatten())\n",
    "    model.add_module(\"dense1\", Dense(num_kernels * output_width**2, \n",
    "                                     dense_hidden_size, \n",
    "                                     init_bound = 0.1632993161855452))\n",
    "    model.add_module(\"relu3\", ReLU())\n",
    "    model.add_module(\"dense2\", Dense(dense_hidden_size, output_classes, \n",
    "                                     init_bound = 0.2992528008322899))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is now in place to train your CNN! Before trying it on zebras, you first try it on a simpler data set where a \"positive\" example looks like this:\n",
    "\n",
    "![positive](./img/positive.png)\n",
    "\n",
    "and a \"negative\" example looks like this:\n",
    "\n",
    "![negative](./img/negative.png)\n",
    "\n",
    "In other words, positive examples contain a three pixel diagonal line going downward right, while negative examples contain a three pixel diagonal line going downward left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamanager import DataPartition, DataManager\n",
    "from training import nlog_softmax_loss, minibatch_training\n",
    "from torch import optim\n",
    "\n",
    "def run(data_config, n_epochs, num_kernels, \n",
    "        kernel_size, dense_hidden_size, \n",
    "        use_maxpool, use_torch_conv_layer):    \n",
    "    \"\"\"\n",
    "    Runs a training regime for a CNN.\n",
    "    \n",
    "    \"\"\"\n",
    "    train_set = DataPartition(data_config, './data', 'train')\n",
    "    test_set = DataPartition(data_config, './data', 'test')\n",
    "    manager = DataManager(train_set, test_set)\n",
    "    loss = nlog_softmax_loss\n",
    "    learning_rate = .001\n",
    "    image_width = 64\n",
    "    net = create_cnn(num_kernels = num_kernels, kernel_size= kernel_size, \n",
    "                                 output_classes=2, image_width=image_width,\n",
    "                                 dense_hidden_size=dense_hidden_size,\n",
    "                                 use_maxpool = use_maxpool,\n",
    "                                 use_torch_conv_layer = use_torch_conv_layer)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "    best_net, monitor = minibatch_training(net, manager, \n",
    "                                           batch_size=32, n_epochs=n_epochs, \n",
    "                                           optimizer=optimizer, loss=loss)\n",
    "    classifier = Classifier(best_net, num_kernels, kernel_size, \n",
    "                            dense_hidden_size, manager.categories, image_width)\n",
    "    return classifier, monitor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def experiment1():\n",
    "    return run('stripes.data.json', \n",
    "               n_epochs = 20,\n",
    "               num_kernels = 20, \n",
    "               kernel_size = 3, \n",
    "               dense_hidden_size = 64,\n",
    "               use_maxpool = True,\n",
    "               use_torch_conv_layer = True)\n",
    "\n",
    "classifier, _ = experiment1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see it reach 100% test accuracy, which means it's time to identify some zebras! Though the training may be a little bit slow.\n",
    "\n",
    "Luckily, while you were at the airport, you sent your code to your UNCOOL team. During your flight, they optimized it and also got it accepted into the torch.nn package as torch.nn.Conv2d. You try out their implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment2():\n",
    "    return run('stripes.data.json', \n",
    "               n_epochs = 20,\n",
    "               num_kernels = 20, \n",
    "               kernel_size = 3, \n",
    "               dense_hidden_size = 64,\n",
    "               use_maxpool = True,\n",
    "               use_torch_conv_layer = True)\n",
    "\n",
    "classifier, _ = experiment2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty fast! Fast enough for more in-depth experimentation. You figure it's worth evaluating the importance of the maxpool layers. Luckily, you've set up your code so that you can train without them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment3():\n",
    "    return run('stripes.data.json', \n",
    "               n_epochs = 20,\n",
    "               num_kernels = 20, \n",
    "               kernel_size= 3, \n",
    "               dense_hidden_size=64, \n",
    "               use_maxpool=False,\n",
    "               use_torch_conv_layer = True)\n",
    "\n",
    "classifier, _ = experiment3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It probably doesn't do exceptionally well, because it's harder for the network to generalize about the diagonal patterns without the maxpooling. It's also slower, because without the maxpooling, the input to the dense layers is much larger.\n",
    "\n",
    "Lesson learned: maxpool is maxcool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify zebras instead of random stripe patterns, all you need to do is swap out the stripes data for the zebra data. Also, given the complexity of the task, you increase the kernel size from 3 to 7. Each epoch should take around 20 seconds to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment4():\n",
    "    return run('zebra.data.json', \n",
    "               n_epochs = 8,\n",
    "               num_kernels = 20, \n",
    "               kernel_size = 7, \n",
    "               dense_hidden_size = 64,\n",
    "               use_maxpool = True,\n",
    "               use_torch_conv_layer = True)\n",
    "\n",
    "classifier, _ = experiment4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you're seeing performance in the low to mid nineties, which is a new state-of-the-art in zebra recognition. The generals will surely be pleased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, your team has created an application called ZebraShop that allows you to run your zebra detector interactively. First, save your classifier from ```experiment3```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('zc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a file called ```zc.json``` that stores your classifier. You can then run the GUI by typing the following from the home directory of this project:\n",
    "\n",
    "    python zebrashop.py zc.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out different images to see where things go right and wrong. In particular, you may want to try out some giraffes. Why do you think they get classified incorrectly? What else gets classified incorrectly?\n",
    "\n",
    "What could you do to improve the system?\n",
    "\n",
    "Feel free to try and make improvements as part of this project submission. If you do something you think is cool, let me know by describing it in the README and/or the pull request message.\n",
    "\n",
    "**Project 4 is now complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
